{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter search\n",
    "\n",
    "For this exercise, you will use cross-validation with a grid-search to optimise the hyperparameters for probabilistic classification of mode choice using Random Forests and Gradient Boosting Decision Trees. \n",
    "\n",
    "You will first optimise the model parameters using only the `train_validate` data, and evaluate the model performance on the `test` data.\n",
    "\n",
    "**Note**: remember, grid-searches are very intensive and can take a long time to run. Always check your code is working by obtaining results for a simple reduced grid first, before running your code on the full grid.\n",
    "\n",
    "Tasks:\n",
    "\n",
    "1. Investigate the documentation for `GridSearchCV` in *scikit-learn*\n",
    "\n",
    "\n",
    "2. Use `GridSearchCV` to optimise the hyperparameters for the `RandomForestClassifier` on the London Passenger Mode Choice dataset\n",
    "    * Use 5-fold cross-validation.\n",
    "    * Evaluate the model performance using the negative log likelihood during the grid search.\n",
    "    * Search for values of *maximum tree depth* over an appropriate range (e.g. between 2 and 15).\n",
    "    * Search for ensemble sizes over an appropriate range (e.g. between 100 and 1000 trees). \n",
    "    * Note that the search may be very slow -  you will have to set up your search with an appropriate grid size.\n",
    "    * Save the trained object as a pickle file to avoid having to rerun the gridsearch multiple times. \n",
    "\n",
    "    \n",
    "3. Investigate the `cv_results_` attribute of the fitted `GridSearchCV` object\n",
    "    * Comment on the returned values: \n",
    "        * What is used to determine the `best_parameters_`/`rank_test_score` in the search? \n",
    "        * What other returned values are important?\n",
    "        * How could these results be visualised?\n",
    "    * Select the parameters from the grid-search that you believe to be optimal, and *if necessary* refit the model to all of the `train_validate` data using these hyperparameters\n",
    "\n",
    "\n",
    "4. Test the fitted model on the `test` data.\n",
    "    * How does the log-likelihood score differ for the test data compared to the cross-validation score?\n",
    "    * What are the possible reasons for any discrepancy? (We will discuss this in detail next week!)\n",
    "\n",
    "\n",
    "### Extension exercise\n",
    "\n",
    "For the extenstion exercise, you will optimise the `xgboost` classifer using the `early_stopping_rounds` variable.\n",
    "\n",
    "\n",
    "* In order to perform cross-validation with early stopping for the `xgboost` classifier, you will need to use the `xgboost.cv` method from the xgboost learning api, documented [here](https://xgboost.readthedocs.io/en/latest/python/python_api.html#module-xgboost.training). \n",
    "    * Again, use 5-fold CV for the search. \n",
    "    * Set `metrics='mlogloss'` to evaluate the model on the log-likelihood loss. \n",
    "    * Note that `xgboost.cv` is **not** compatible with `GridSearchCV` in *scikit-learn*. As such, you will need to setup the search manually, as we did in class. \n",
    "\n",
    "\n",
    "* Set `num_boost_round=1000` and `early_stopping_rounds=10`, so that the number of boosting rounds is capped at 1000, but stops earlier if the model performance does not increase for 10 rounds. \n",
    "\n",
    "\n",
    "* Fix the learning rate (called `eta` in learning API) to be 0.1, and investigate appropriate values of `max_depth`.\n",
    "\n",
    "\n",
    "* Test the optimised model on the `test` data, and comment on the results. \n",
    "\n",
    "\n",
    "* Remember to use Google/Stack Overflow for help! This has been attempted many times before. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GridSearchCV and RF\n",
    "\n",
    "We first use the `GridSearchCV` to optimise the parameters for the `RandomForestClassifier`. \n",
    "\n",
    "We will first import necessary libraries..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "import time\n",
    "import xgboost as xgb\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...and do the usual data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train_validate = pd.read_csv('data/train_validate.csv', index_col='trip_id')\n",
    "\n",
    "target = ['travel_mode']\n",
    "id_context = ['trip_id', \n",
    "              'household_id', \n",
    "              'person_n', \n",
    "              'trip_n',\n",
    "              'survey_year',\n",
    "              'travel_year'\n",
    "             ]\n",
    "features = [c for c in df_train_validate.columns \n",
    "            if c not in (target + id_context)]\n",
    "\n",
    "y_train_validate = df_train_validate[target].values.ravel()\n",
    "X_train_validate = df_train_validate[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can create an instance of the Random Forest Classifier, specifying all of the parameters that won't be modified during the search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForestClassifier(criterion='entropy', \n",
    "                            random_state=42,\n",
    "                            n_jobs=-1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can specify the search space for the Grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'n_estimators': [50, 100, 200, 500, 1000],\n",
    "              'max_depth': list(range(2,16,2))}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search is a *class*, which is used like any other model in *scikit-learn*. First we create an instance of the class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = GridSearchCV(rf, parameters, scoring='neg_log_loss', cv=5, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can fit the `GridSearchCV` instance we have created. This may take a considerable ammount of time. Remember, it is relatively straightforward to parallelise a grid search over multiple computers, and combine results after. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 35 candidates, totalling 175 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "[Parallel(n_jobs=1)]: Done 175 out of 175 | elapsed: 18.5min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=RandomForestClassifier(criterion='entropy', n_jobs=-1,\n",
       "                                              random_state=42),\n",
       "             param_grid={'max_depth': [2, 4, 6, 8, 10, 12, 14],\n",
       "                         'n_estimators': [50, 100, 200, 500, 1000]},\n",
       "             scoring='neg_log_loss', verbose=1)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train_validate, y_train_validate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The \"best score\" and associated parameters are stores as attributes of the `GridSearchCV` object. Note that the underscore at the end of the attribute indicates that this attribute is only available once the object has been fitted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score: -0.662 with parameters {'max_depth': 14, 'n_estimators': 1000}\n"
     ]
    }
   ],
   "source": [
    "print(f'Best score: {clf.best_score_:.3f} with parameters {clf.best_params_}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can investigate the `cv_results_` attribute to see more detail of what is stored during the search. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_fit_time': array([ 0.77885876,  0.67633042,  1.15194407,  2.54367452,  4.98540425,\n",
       "         0.57866468,  0.979353  ,  1.75176353,  4.20671277,  9.98450956,\n",
       "         0.89696541,  1.44573307,  2.91394386,  6.40828648, 12.45935192,\n",
       "         0.95720854,  1.84413424,  3.15651121,  7.73939381, 15.43058615,\n",
       "         1.14947162,  2.01956511,  3.85093932,  9.34747758, 20.2559443 ,\n",
       "         1.42939262,  2.52801294,  4.92684979, 11.78022909, 22.22877393,\n",
       "         1.3956141 ,  2.61734524,  5.35444336, 13.01463552, 25.23190522]),\n",
       " 'std_fit_time': array([0.7309393 , 0.0383258 , 0.03012608, 0.04514808, 0.06288824,\n",
       "        0.02430845, 0.02386224, 0.04651695, 0.07347834, 1.24083135,\n",
       "        0.06021578, 0.16355033, 0.28825183, 0.43865496, 0.59596438,\n",
       "        0.04509335, 0.28436911, 0.12941174, 0.27088153, 0.46945876,\n",
       "        0.08765582, 0.09668774, 0.13074995, 0.08050457, 0.73157468,\n",
       "        0.10453383, 0.05740265, 0.09592768, 0.64558709, 0.18608215,\n",
       "        0.02610772, 0.07200383, 0.30801553, 0.44153743, 0.07615669]),\n",
       " 'mean_score_time': array([0.12738156, 0.21875968, 0.23844924, 0.52445502, 1.00386496,\n",
       "        0.11249614, 0.17355742, 0.28098731, 0.57381787, 1.02253141,\n",
       "        0.0605557 , 0.13569903, 0.22989073, 0.56902175, 1.01992307,\n",
       "        0.09428258, 0.13821754, 0.24588852, 0.5243834 , 0.95227575,\n",
       "        0.09370065, 0.14812961, 0.23904362, 0.51267424, 1.00974364,\n",
       "        0.06726108, 0.12182097, 0.23386021, 0.54515181, 0.96117892,\n",
       "        0.09603553, 0.13874121, 0.23417106, 0.53611975, 0.98260093]),\n",
       " 'std_score_time': array([0.03435332, 0.02964151, 0.0220354 , 0.05073203, 0.06921752,\n",
       "        0.03643225, 0.00920318, 0.02077511, 0.03181428, 0.08786214,\n",
       "        0.00226119, 0.02198434, 0.02196435, 0.0470535 , 0.03200912,\n",
       "        0.00987813, 0.02189349, 0.01291387, 0.00286827, 0.01309541,\n",
       "        0.00995933, 0.01177729, 0.0029341 , 0.00964816, 0.02480825,\n",
       "        0.00217799, 0.0070506 , 0.01753739, 0.02958291, 0.02953654,\n",
       "        0.01052368, 0.01019726, 0.00963187, 0.00607898, 0.02485461]),\n",
       " 'param_max_depth': masked_array(data=[2, 2, 2, 2, 2, 4, 4, 4, 4, 4, 6, 6, 6, 6, 6, 8, 8, 8,\n",
       "                    8, 8, 10, 10, 10, 10, 10, 12, 12, 12, 12, 12, 14, 14,\n",
       "                    14, 14, 14],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'param_n_estimators': masked_array(data=[50, 100, 200, 500, 1000, 50, 100, 200, 500, 1000, 50,\n",
       "                    100, 200, 500, 1000, 50, 100, 200, 500, 1000, 50, 100,\n",
       "                    200, 500, 1000, 50, 100, 200, 500, 1000, 50, 100, 200,\n",
       "                    500, 1000],\n",
       "              mask=[False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False, False, False, False, False, False,\n",
       "                    False, False, False],\n",
       "        fill_value='?',\n",
       "             dtype=object),\n",
       " 'params': [{'max_depth': 2, 'n_estimators': 50},\n",
       "  {'max_depth': 2, 'n_estimators': 100},\n",
       "  {'max_depth': 2, 'n_estimators': 200},\n",
       "  {'max_depth': 2, 'n_estimators': 500},\n",
       "  {'max_depth': 2, 'n_estimators': 1000},\n",
       "  {'max_depth': 4, 'n_estimators': 50},\n",
       "  {'max_depth': 4, 'n_estimators': 100},\n",
       "  {'max_depth': 4, 'n_estimators': 200},\n",
       "  {'max_depth': 4, 'n_estimators': 500},\n",
       "  {'max_depth': 4, 'n_estimators': 1000},\n",
       "  {'max_depth': 6, 'n_estimators': 50},\n",
       "  {'max_depth': 6, 'n_estimators': 100},\n",
       "  {'max_depth': 6, 'n_estimators': 200},\n",
       "  {'max_depth': 6, 'n_estimators': 500},\n",
       "  {'max_depth': 6, 'n_estimators': 1000},\n",
       "  {'max_depth': 8, 'n_estimators': 50},\n",
       "  {'max_depth': 8, 'n_estimators': 100},\n",
       "  {'max_depth': 8, 'n_estimators': 200},\n",
       "  {'max_depth': 8, 'n_estimators': 500},\n",
       "  {'max_depth': 8, 'n_estimators': 1000},\n",
       "  {'max_depth': 10, 'n_estimators': 50},\n",
       "  {'max_depth': 10, 'n_estimators': 100},\n",
       "  {'max_depth': 10, 'n_estimators': 200},\n",
       "  {'max_depth': 10, 'n_estimators': 500},\n",
       "  {'max_depth': 10, 'n_estimators': 1000},\n",
       "  {'max_depth': 12, 'n_estimators': 50},\n",
       "  {'max_depth': 12, 'n_estimators': 100},\n",
       "  {'max_depth': 12, 'n_estimators': 200},\n",
       "  {'max_depth': 12, 'n_estimators': 500},\n",
       "  {'max_depth': 12, 'n_estimators': 1000},\n",
       "  {'max_depth': 14, 'n_estimators': 50},\n",
       "  {'max_depth': 14, 'n_estimators': 100},\n",
       "  {'max_depth': 14, 'n_estimators': 200},\n",
       "  {'max_depth': 14, 'n_estimators': 500},\n",
       "  {'max_depth': 14, 'n_estimators': 1000}],\n",
       " 'split0_test_score': array([-0.87258693, -0.87207   , -0.86900383, -0.86964431, -0.87039777,\n",
       "        -0.76907177, -0.76582458, -0.76294127, -0.76337756, -0.76273475,\n",
       "        -0.71292182, -0.7126255 , -0.71250402, -0.71338376, -0.71367991,\n",
       "        -0.68492389, -0.6844062 , -0.68404617, -0.68403814, -0.68371921,\n",
       "        -0.67130412, -0.66855168, -0.66758557, -0.66630811, -0.66622626,\n",
       "        -0.66510111, -0.66163859, -0.65929479, -0.65765131, -0.65709948,\n",
       "        -0.66690858, -0.65788567, -0.65474207, -0.65342746, -0.65245578]),\n",
       " 'split1_test_score': array([-0.87247654, -0.87102926, -0.86866823, -0.86975995, -0.87007961,\n",
       "        -0.76672267, -0.76469391, -0.76260025, -0.76383147, -0.76232132,\n",
       "        -0.71620095, -0.71373056, -0.71332678, -0.71336751, -0.71348744,\n",
       "        -0.68789749, -0.68665858, -0.68643419, -0.68556329, -0.68563502,\n",
       "        -0.67237075, -0.67150353, -0.67046402, -0.67028644, -0.66963408,\n",
       "        -0.67041847, -0.66721541, -0.66477445, -0.66396679, -0.66284776,\n",
       "        -0.68793713, -0.67335293, -0.66899835, -0.6629365 , -0.66195759]),\n",
       " 'split2_test_score': array([-0.86635406, -0.8648743 , -0.86150375, -0.86284735, -0.8632194 ,\n",
       "        -0.7597903 , -0.75891644, -0.75722709, -0.75844456, -0.75683269,\n",
       "        -0.70750691, -0.70743779, -0.70695803, -0.70764345, -0.70848989,\n",
       "        -0.68371181, -0.68224563, -0.68144863, -0.68078095, -0.68037352,\n",
       "        -0.66747061, -0.66498129, -0.6643924 , -0.6641087 , -0.66379924,\n",
       "        -0.66436959, -0.66084471, -0.6571742 , -0.65573508, -0.65546137,\n",
       "        -0.67312898, -0.66387038, -0.65710409, -0.65600657, -0.65481457]),\n",
       " 'split3_test_score': array([-0.8882938 , -0.88647812, -0.88302199, -0.88322958, -0.88330404,\n",
       "        -0.78816037, -0.78608535, -0.78308996, -0.78400747, -0.78346122,\n",
       "        -0.7395164 , -0.73923604, -0.73891043, -0.73904642, -0.7390468 ,\n",
       "        -0.71293315, -0.71256695, -0.71182236, -0.71151678, -0.71169987,\n",
       "        -0.69803338, -0.69698118, -0.69534556, -0.69491357, -0.69496093,\n",
       "        -0.69269215, -0.68950189, -0.68869108, -0.6864575 , -0.68615884,\n",
       "        -0.69837132, -0.6917684 , -0.68773121, -0.68352628, -0.68327755]),\n",
       " 'split4_test_score': array([-0.87949066, -0.8769052 , -0.87367754, -0.87425377, -0.8745238 ,\n",
       "        -0.77215571, -0.77065316, -0.76741197, -0.76922223, -0.76837701,\n",
       "        -0.7208174 , -0.71972821, -0.71901292, -0.71932646, -0.71992167,\n",
       "        -0.69295793, -0.69201978, -0.68985481, -0.68977472, -0.68960802,\n",
       "        -0.67569503, -0.67281834, -0.6717162 , -0.67127136, -0.67098689,\n",
       "        -0.67256168, -0.66459783, -0.66179294, -0.66153205, -0.66157995,\n",
       "        -0.68494284, -0.6677097 , -0.66273983, -0.65780278, -0.65785657]),\n",
       " 'mean_test_score': array([-0.8758404 , -0.87427138, -0.87117507, -0.87194699, -0.87230492,\n",
       "        -0.77118016, -0.76923469, -0.76665411, -0.76777666, -0.7667454 ,\n",
       "        -0.7193927 , -0.71855162, -0.71814244, -0.71855352, -0.71892514,\n",
       "        -0.69248485, -0.69157943, -0.69072123, -0.69033478, -0.69020713,\n",
       "        -0.67697478, -0.6749672 , -0.67390075, -0.67337764, -0.67312148,\n",
       "        -0.6730286 , -0.66875969, -0.66634549, -0.66506855, -0.66462948,\n",
       "        -0.68225777, -0.67091742, -0.66626311, -0.66273992, -0.66207241]),\n",
       " 'std_test_score': array([0.00748736, 0.00720561, 0.00708724, 0.00671576, 0.00658848,\n",
       "        0.00941618, 0.00921597, 0.00882949, 0.00880379, 0.00912141,\n",
       "        0.01095741, 0.01105539, 0.01106472, 0.01089227, 0.01069439,\n",
       "        0.01071014, 0.01098599, 0.01090802, 0.0109789 , 0.01115342,\n",
       "        0.01085159, 0.01133232, 0.01101593, 0.01107863, 0.01120828,\n",
       "        0.01031028, 0.01061416, 0.01145726, 0.01107553, 0.01110624,\n",
       "        0.01111795, 0.01157991, 0.01181014, 0.01084939, 0.01106888]),\n",
       " 'rank_test_score': array([35, 34, 31, 32, 33, 30, 29, 26, 28, 27, 25, 22, 21, 23, 24, 20, 19,\n",
       "        18, 17, 16, 14, 13, 12, 11, 10,  9,  7,  6,  4,  3, 15,  8,  5,  2,\n",
       "         1], dtype=int32)}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.cv_results_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ranking is determined based purely on the mean test score (in this case the negative log likelihood), calculated over the 5 cross-validation folds. \n",
    "\n",
    "As well as the mean score, the *standard deviation* (*std*) of the scores and the *fit times* and *score times* are also important. \n",
    "\n",
    "The standard deviation of the scores could be used to investigate whether the performance differences between different parameters are statistically significant. Note that this is not commonly applied in practice. \n",
    "\n",
    "We can see there is a large variation of the score and fit times depending on the model parameters, with small ensembles having much quicker score and fit times than larger ensembles. Knowing the fit and score times can allow the modeller to make a more informed decision on approrpriate model parameters, penalising parameter combinations with very high times. \n",
    "\n",
    "Whilst the fit times are larger than the score times, it may be desirable to have prioritise low score times if the model is to be used for real time prediction on large data quantities.\n",
    "\n",
    "For now, we will simply use the parameters with the best mean log likelihood. By default, the model is automatically retrained on all of the train/validate data with these parameters at the end of the Grid Search. This can be obtained using the appribute `best_estimator_`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = clf.best_estimator_\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test the final model, we must import and provess the holdout test data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test = pd.read_csv('data/test.csv', index_col='trip_id')\n",
    "\n",
    "y_test = df_test[target].travel_mode.values\n",
    "X_test = df_test[features]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can output predicted probabilities for the finak model on the test data and compute the cross entropy loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = rf.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6776073815996025"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_test, pred_proba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By comparing the test score with the cross-validation score, we can see that the cross-validation has clearly overestimated the performance of the classifier. We will discuss the reasons for this in the next lecture. \n",
    "\n",
    "This highlights the need to perform true external validation on a holdout test sample collected separately from the training data. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation and early stopping with XGBoost\n",
    "\n",
    "We will now perform a similar grid search for the `xgboost` classifier. However, due to the sequential nature of boosting, we can use early stopping to determine the appropriate ensmble size automatically. We therefore do not need to include ensemble size in the grid itself. \n",
    "\n",
    "In order to make use of the `cv` from `xgboost`, we will have to transform to a `DMatrix` object, within the `xgboost` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgtrain = xgb.DMatrix(X_train_validate, label=y_train_validate)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can then set up the grid search manually, as we did in class, but making use of the `cv` method to perform cross-validation with the classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CV for max_depth = 2\n",
      "CV for max_depth = 4\n",
      "CV for max_depth = 6\n",
      "CV for max_depth = 8\n",
      "CV for max_depth = 10\n",
      "CV for max_depth = 12\n",
      "CV for max_depth = 14\n"
     ]
    }
   ],
   "source": [
    "max_depths = list(range(2, 15, 2))\n",
    "\n",
    "res = {'scores': [], 'n_estimators': [], 'params': [], 'full_results': []}\n",
    "\n",
    "for d in max_depths:\n",
    "    \n",
    "    print(f'CV for max_depth = {d}')\n",
    "    \n",
    "    clf = xgb.XGBClassifier(objective='multi:softprob', \n",
    "                            max_depth=d,\n",
    "                            eta=0.1,\n",
    "                            n_jobs=-1)\n",
    "    \n",
    "    xgb_param = clf.get_xgb_params()\n",
    "    \n",
    "    xgb_param['num_class'] = 4\n",
    "\n",
    "    cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=1000, nfold=5,\n",
    "                      metrics=['mlogloss'], early_stopping_rounds=10, \n",
    "                      verbose_eval=False, seed=42)\n",
    "    \n",
    "    res['scores'].append(min(cvresult['test-mlogloss-mean']))\n",
    "    res['n_estimators'].append(cvresult['test-mlogloss-mean'].idxmin())\n",
    "    res['params'].append(xgb_param)\n",
    "    res['full_results'].append(cvresult)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best log loss found: 0.496\n"
     ]
    }
   ],
   "source": [
    "print(f'Best log loss found: {np.min(res[\"scores\"]):.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters: {'objective': 'multi:softprob', 'base_score': None, 'booster': None, 'colsample_bylevel': None, 'colsample_bynode': None, 'colsample_bytree': None, 'gamma': None, 'gpu_id': None, 'interaction_constraints': None, 'learning_rate': None, 'max_delta_step': None, 'max_depth': 10, 'min_child_weight': None, 'monotone_constraints': None, 'n_jobs': -1, 'num_parallel_tree': None, 'random_state': None, 'reg_alpha': None, 'reg_lambda': None, 'scale_pos_weight': None, 'subsample': None, 'tree_method': None, 'validate_parameters': None, 'verbosity': None, 'eta': 0.1, 'num_class': 4}\n",
      "298 rounds of boosting needed\n"
     ]
    }
   ],
   "source": [
    "best_run = np.argmin(res[\"scores\"])\n",
    "best_params = res['params'][best_run]\n",
    "best_n = res['n_estimators'][best_run]+1\n",
    "\n",
    "print(f'Best parameters: {best_params}')\n",
    "print(f'{best_n} rounds of boosting needed')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now fit the model on all of the data with the best `max_depth` and test it on the test data as we did with the previous model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/timmy/miniconda3/envs/learn/lib/python3.8/site-packages/xgboost/sklearn.py:888: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
      "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20:59:29] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:541: \n",
      "Parameters: { label_encoder } might not be used.\n",
      "\n",
      "  This may not be accurate due to some parameters are only used in language bindings but\n",
      "  passed down to XGBoost core.  Or some parameters are not used but slip through this\n",
      "  verification. Please open an issue if you find above cases.\n",
      "\n",
      "\n",
      "[20:59:29] WARNING: /tmp/build/80754af9/xgboost-split_1619724447847/work/src/learner.cc:1061: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=1, eta=0.1, gamma=0,\n",
       "              gpu_id=-1, importance_type='gain', interaction_constraints='',\n",
       "              label_encoder=False, learning_rate=0.100000001, max_delta_step=0,\n",
       "              max_depth=10, min_child_weight=1, missing=nan,\n",
       "              monotone_constraints='()', n_estimators=298, n_jobs=-1,\n",
       "              num_class=4, num_parallel_tree=1, objective='multi:softprob',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=None,\n",
       "              subsample=1, tree_method='exact', validate_parameters=1,\n",
       "              verbosity=None)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = xgb.XGBClassifier(**best_params, n_estimators=best_n,label_encoder=False)\n",
    "\n",
    "clf.fit(X_train_validate, y_train_validate)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_proba = clf.predict_proba(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7496789120988846"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_test, pred_proba)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the Random Forst classifier, there is a large discrpeancy between the estimated performance using cross-validation and the true out-of-sample performance, but here the difference is even greater."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
